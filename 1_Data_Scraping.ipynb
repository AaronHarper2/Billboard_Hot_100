{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the list of Billboard Hot 100 songs from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(year):\n",
    "    url = f\"https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_{year}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "    data = []\n",
    "    if table:\n",
    "        rows = table.find_all('tr')[1:]\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 3:\n",
    "                rank = cells[0].text.strip()\n",
    "                title = cells[1].text.strip()\n",
    "                artist = cells[2].text.strip()\n",
    "                data.append({'Rank': rank, 'Title': title, 'Artist': artist})\n",
    "    return data\n",
    "\n",
    "billboard = pd.DataFrame(columns=['Rank', 'Title', 'Artist'])\n",
    "columns = ['Rank', 'Title', 'Artist']\n",
    "\n",
    "for year in range(2018, 2024):\n",
    "    year_data = scrape(year)\n",
    "    year_df = pd.DataFrame(year_data, columns=columns)\n",
    "    year_df['Year'] = year\n",
    "    billboard = pd.concat([billboard, year_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_no_artist(year):\n",
    "    url = f\"https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_{year}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "    data = []\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        if len(columns) >= 2:\n",
    "            rank = columns[0].text.strip()\n",
    "            title = columns[1].text.strip()\n",
    "            data.append({'Year': year, 'Rank': rank, 'Title': title})\n",
    "    return data\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for year in range(2018, 2024):\n",
    "    year_data = scrape_no_artist(year)\n",
    "    all_data.extend(year_data)\n",
    "\n",
    "no_artists = pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Billboard100 = pd.merge(no_artists, billboard, on=['Year', 'Rank'], how='left')\n",
    "Billboard100['Artist'].fillna(method='ffill', inplace=True)\n",
    "Billboard100.drop(columns=['Title_y'], inplace=True)\n",
    "Billboard100 = Billboard100.rename(columns={'Title_x': 'Title', 'Artist_x': 'Artist'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_song(x):\n",
    "    x = x.replace(\"-\", \" \")\n",
    "    x = x.replace(\"'\", \" \")\n",
    "    x = re.sub(r'[^\\w\\s]','', x)\n",
    "    x = unidecode.unidecode(x)\n",
    "    return x\n",
    "\n",
    "Billboard100[['Artist']]\\\n",
    "    = Billboard100[['Artist']].applymap(\n",
    "        lambda x: x.replace('&', 'and'))\n",
    "\n",
    "Billboard100[['song_clean', 'artist_clean']]\\\n",
    "    = Billboard100[['Title', 'Artist']].applymap(clean_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Billboard100['song_clean'] = Billboard100['song_clean'].apply(lambda x: x.lower())\n",
    "Billboard100['artist_clean'] = Billboard100['artist_clean'].apply(lambda x: x.lower())\n",
    "Billboard100['artist_clean'] = Billboard100['artist_clean'].str.replace('featuring', 'feat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the list to obtain the lyrics from Lyrics Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        lyrics_div = soup.find('p', class_='songLyricsV14')\n",
    "        \n",
    "        if lyrics_div:\n",
    "            lyrics = lyrics_div.get_text(\"\\n\")\n",
    "            return lyrics.strip()\n",
    "        else:\n",
    "            return \"Lyrics not found\"\n",
    "    else:\n",
    "        return \"Failed to get lyrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_lyrics(artist, song):\n",
    "    search_url = f\"https://www.songlyrics.com/{artist}/{song}-lyrics/\"\n",
    "    return get_lyrics(search_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Billboard100['Lyrics'] = \"\"\n",
    "\n",
    "for index, row in Billboard100.iterrows():\n",
    "    artist_clean = row['artist_clean']\n",
    "    song_clean = row['song_clean']\n",
    "    lyrics = fetch_lyrics(artist_clean, song_clean)\n",
    "    Billboard100.at[index, 'Lyrics'] = lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Billboard100['Lyrics'] = Billboard100['Lyrics'].fillna('')\n",
    "Billboard100['Lyrics'] = Billboard100['Lyrics'].apply(lambda x: re.sub(r'\\[.*?\\]', '', str(x)))\n",
    "Billboard100['Lyrics'] = Billboard100['Lyrics'].apply(lambda x: x.replace('\\n', ' ')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_songs = Billboard100[(Billboard100['Lyrics'] == \"Failed to get lyrics\") | (Billboard100['Lyrics'] == \"Lyrics not found\")]\n",
    "Billboard100 = Billboard100.drop(missed_songs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_artist(artist):\n",
    "    parts = re.split(r',| and | featuring ', artist, flags=re.IGNORECASE)\n",
    "    return parts[0].strip().lower()\n",
    "\n",
    "missed_songs['artist_clean'] = missed_songs['Artist'].apply(extract_artist)\n",
    "missed_songs['artist_clean'] = missed_songs['artist_clean'].str.replace('the ', '')\n",
    "missed_songs['artist_clean'] = missed_songs['artist_clean'].str.replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_songs['song_clean'] = missed_songs['song_clean'].replace('bood', \"boo'd\")\n",
    "missed_songs['song_clean'] = missed_songs['song_clean'].replace('lemon', \"lemons\")\n",
    "missed_songs['artist_clean'] = missed_songs['artist_clean'].replace('tones', \"tones and i\")\n",
    "missed_songs['artist_clean'] = missed_songs['artist_clean'].replace('bts', \"bts bangtan boys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_song(x):\n",
    "    x = x.replace(\"-\", \" \")\n",
    "    x = x.replace(\"'\", \"\")\n",
    "    x = re.sub(r'[^\\w\\s]', '', x)\n",
    "    x = unidecode.unidecode(x)\n",
    "    unwanted_words = ['the', 'like', 'a', 'with', 'in', 'for', 'up', 'to', 'at', \n",
    "                      'on', 'that', 'from', 'of', 'but', 'as', 'before', 'is', 'by']\n",
    "    words = x.split()\n",
    "    cleaned_words = [word for word in words if word.lower() not in unwanted_words]\n",
    "    return ' '.join(cleaned_words).lower()\n",
    "\n",
    "missed_songs['song_clean'] = missed_songs['Title'].apply(clean_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_lyrics(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"lxml\")\n",
    "        lyrics_element = soup.find(\"div\", class_=\"ltf\")\n",
    "        if lyrics_element:\n",
    "            lyrics = lyrics_element.get_text(separator=\"\\n\")\n",
    "            return lyrics.strip() \n",
    "    return None\n",
    "\n",
    "def generate_url(artist, song):\n",
    "    artist = artist.replace(\" \", \"-\").lower()\n",
    "    song = song.replace(\" \", \"-\").lower()\n",
    "    return f\"https://lyricstranslate.com/en/{artist}-{song}-lyrics.html\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_songs['Lyrics'] = missed_songs.apply(lambda row: scrape_lyrics(generate_url(row['artist_clean'], row['song_clean'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_songs['Lyrics'] = missed_songs['Lyrics'].fillna('')\n",
    "missed_songs['Lyrics'] = missed_songs['Lyrics'].apply(lambda x: re.sub(r'\\[.*?\\]', '', str(x)))\n",
    "missed_songs['Lyrics'] = missed_songs['Lyrics'].apply(lambda x: x.replace('\\n', ' ')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed_songs = pd.read_csv('missed_songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Billboard100 = Billboard100.drop(columns=['song_clean', 'artist_clean'])\n",
    "missed_songs = missed_songs.drop(columns=['song_clean', 'artist_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_df = pd.concat([Billboard100, missed_songs], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_df = appended_df.sort_values(by=['Year', 'Rank'], ascending=[True, True])\n",
    "appended_df['Lyrics'] = appended_df['Lyrics'].str.lower()\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.strip('\"').lower()\n",
    "\n",
    "appended_df['Title'] = appended_df['Title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_df.to_csv('Billboard100.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
